{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "527ad5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Colab use\n",
    "#from google.colab import files\n",
    "#uploaded = files.upload()\n",
    "#import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836da732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardization\n",
    "def standard_scaler(X_train, X_test):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train) #standardize based on train data\n",
    "    X_test = scaler.transform(X_test) #standardize test data\n",
    "    \n",
    "    filename = '../Dashboard UI/Dashboard UI/model/std_scaler.pkl'\n",
    "    pickle.dump(scaler, open(filename, 'wb')) #save standardization scaler\n",
    "    \n",
    "    #Colab use\n",
    "    #filename = 'std_scaler.pkl'\n",
    "    #pickle.dump(scaler, open(filename, 'wb')) #save standardization scaler\n",
    "    #files.download('std_scaler.pkl')\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83a4c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred, label='test'):\n",
    "    mse = mean_squared_error(y_true, y_pred) #calculate MSE\n",
    "    rmse = np.sqrt(mse) #calculate RMSE\n",
    "    variance = r2_score(y_true, y_pred) #calculate R2\n",
    "    print('{} set RMSE:{}, R2:{}'.format(label, rmse, variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e85eae",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b529c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = ['ID', 'Cycle']\n",
    "setting_names = ['OpSet1', 'OpSet2', 'OpSet3']\n",
    "sensor_names = ['SensorMeasure{}'.format(i) for i in range(1,22)] \n",
    "col_names = index_names + setting_names + sensor_names\n",
    "\n",
    "train_data = pd.read_csv(\"../Data/train_set.csv\")\n",
    "test_data = pd.read_csv(\"../Data/test_set.csv\")\n",
    "true_RUL = pd.read_csv(\"../Data/RUL_FD001.txt\", sep='\\s+', header = None)\n",
    "\n",
    "#Colab use\n",
    "#train_data = pd.read_csv(io.BytesIO(uploaded['train_set.csv']))\n",
    "#test_data = pd.read_csv(io.BytesIO(uploaded['test_set.csv']))\n",
    "#true_RUL = pd.read_csv(io.BytesIO(uploaded['RUL_FD001.txt']), sep='\\s+', header = None)\n",
    "\n",
    "train_RUL = train_data['RUL']\n",
    "train_RUL = train_RUL.clip(upper = 125) #clip maximum cycle at 125\n",
    "test_RUL = test_data['RUL']\n",
    "\n",
    "train_data = train_data.drop(['RUL'], 1)\n",
    "test_data = test_data.drop(['RUL'], 1)\n",
    "\n",
    "test_data = test_data.groupby(['ID'])\n",
    "test_data = test_data.tail(1)\n",
    "\n",
    "#assign to new variable for easy understanding\n",
    "train = train_data\n",
    "train_y = train_RUL\n",
    "test = test_data.groupby(['ID']).tail(1) #get the last record for each engine\n",
    "test_y = true_RUL\n",
    "\n",
    "#only sensor value considered\n",
    "train = train[sensor_names]\n",
    "test = test[sensor_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f78ead",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed71e579",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Principle Component Analysis\n",
    "def pca(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    #for reproducible result\n",
    "    np.random.seed(2)\n",
    "    \n",
    "    X_train, X_test = standard_scaler(X_train, X_test) #standardize data\n",
    "    \n",
    "    #rename columns after standardization\n",
    "    sensor_names = ['SensorMeasure{}'.format(i) for i in range(1,22)] \n",
    "    X_train = pd.DataFrame(X_train, columns = sensor_names).reset_index(drop = True)\n",
    "    X_test = pd.DataFrame(X_test, columns = sensor_names).reset_index(drop = True)\n",
    "\n",
    "    #-------------------------------Fitting Model----------------------------\n",
    "    # Make an instance of the Model\n",
    "    pca = PCA(0.95, random_state = 1) #95% of the variance (information) is retained\n",
    "\n",
    "    x1 = pca.fit_transform(X_train) #PCA based on train data\n",
    "    x2 = pca.transform(X_test) #transform test data\n",
    "    \n",
    "    filename = '../Dashboard UI/Dashboard UI/model/pca.pkl'\n",
    "    pickle.dump(pca, open(filename, 'wb')) #save PCA model\n",
    "    \n",
    "    #Colab use\n",
    "    #filename = 'pca.pkl'\n",
    "    #pickle.dump(pca, open(filename, 'wb')) #save PCA model\n",
    "    #files.download('pca.pkl')\n",
    "    \n",
    "    #Graph to indicate information contains in each component\n",
    "    #fir = plt.figure(figsize=(8,5))\n",
    "    #sing_vals = np.arange(len(pca.components_)) + 1\n",
    "    #plt.plot(sing_vals, pca.explained_variance_ratio_, 'ro-', linewidth=2)\n",
    "    #plt.title('Scree Plot', fontsize = 20)\n",
    "    #plt.xlabel('Principal Component', fontsize = 20)\n",
    "    #plt.ylabel('Eigenvalue', fontsize = 20)\n",
    "    #plt.xticks(fontsize=10)\n",
    "    #plt.yticks(fontsize=10)\n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    #convert to data frame\n",
    "    X_train = pd.DataFrame(data = x1)\n",
    "    X_test = pd.DataFrame(data = x2)\n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d7a3d",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "754c8ac4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def gradient_boosting_regressor(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    #for reproducible result\n",
    "    np.random.seed(2)\n",
    "    \n",
    "    #------------------------------Train Model-------------------------------\n",
    "    #GBR model for randomised search\n",
    "    #GBR = GradientBoostingRegressor(random_state=1)\n",
    "    \n",
    "    #parameter after first grid search\n",
    "    #GBR = GradientBoostingRegressor(learning_rate = 0.1, \n",
    "                                    #n_estimators = 100,\n",
    "                                    #subsample = 0.6,\n",
    "                                    #random_state=1)\n",
    "    \n",
    "    #parameter after second grid search\n",
    "    #GBR = GradientBoostingRegressor(learning_rate = 0.1, \n",
    "                                    #n_estimators = 100,\n",
    "                                    #subsample = 0.6,\n",
    "                                    #min_samples_split = 1400,\n",
    "                                    #max_depth = 5,\n",
    "                                    #random_state=1)\n",
    "    \n",
    "    #parameter after third grid search\n",
    "    GBR = GradientBoostingRegressor(learning_rate = 0.1, \n",
    "                                    n_estimators = 100,\n",
    "                                    subsample = 0.6,\n",
    "                                    min_samples_split = 1400,\n",
    "                                    max_depth = 5,\n",
    "                                    min_samples_leaf = 1,\n",
    "                                    random_state=1)\n",
    "\n",
    "    #shrinks the contribution of each tree (0.1)\n",
    "    learning_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "    #The number of boosting stages to perform (100)\n",
    "    n_estimators = list(range(40, 200, 20))\n",
    "    #The fraction of samples to be used for fitting the individual base learners\n",
    "    subsample = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    \n",
    "    #minimum number of samples required to split an internal node (2)\n",
    "    min_samples_split = list(range(1000,10000,400))\n",
    "    #Maximum depth of the individual regression estimators (3)\n",
    "    max_depth = list(range(1,10,2))\n",
    "    #max_depth.append(None)\n",
    "    \n",
    "    #minimum number of samples required to be at a leaf node (1)\n",
    "    min_samples_leaf = list([1])\n",
    "    \n",
    "    \n",
    "    #param_grid = {'learning_rate': learning_rate,\n",
    "                  #'n_estimators': n_estimators,\n",
    "                  #'subsample': subsample,\n",
    "                  #'min_samples_split': min_samples_split,\n",
    "                  #'max_depth': max_depth, \n",
    "                  #'min_samples_leaf': min_samples_leaf}\n",
    "    \n",
    "    # Create the dictionary\n",
    "    #first grid search\n",
    "    #param_grid = {'learning_rate': learning_rate,\n",
    "                  #'n_estimators': n_estimators,\n",
    "                  #'subsample': subsample\n",
    "                 #}\n",
    "    #second grid search\n",
    "    #param_grid = {'min_samples_split': min_samples_split,\n",
    "                  #'max_depth': max_depth\n",
    "                 #}\n",
    "    #third grid search\n",
    "    #param_grid = {'min_samples_leaf': min_samples_leaf}\n",
    "    \n",
    "    #K-fold cross validation\n",
    "    #cv = KFold(n_splits=10, shuffle = True, random_state = 1)\n",
    "    \n",
    "    #grid search \n",
    "    #grid_search = GridSearchCV(GBR, param_grid = param_grid, \n",
    "                               #cv = cv, n_jobs=-1)\n",
    "    \n",
    "    #grid_search.fit(X_train, y_train.values.ravel()) #train model using grid search\n",
    "\n",
    "    GBR.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    #rand_search = RandomizedSearchCV(GBR, param_distributions = param_grid, \n",
    "                                     #cv = cv, n_jobs=-1, random_state=1)\n",
    "    #rand_search.fit(X_train, y_train.values.ravel()) #train model using randomised search\n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    #------------------------------Predict X_test----------------------------\n",
    "    #y_pred_train = rand_search.predict(X_train) \n",
    "    #y_pred_test = rand_search.predict(X_test)\n",
    "    \n",
    "    #y_pred_train = grid_search.predict(X_train)\n",
    "    #y_pred_test = grid_search.predict(X_test)\n",
    "    \n",
    "    y_pred_train = GBR.predict(X_train) #predict on train data\n",
    "    y_pred_test = GBR.predict(X_test) #predict on tets data\n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    #---------------------------------Accuracy-------------------------------\n",
    "    # Use score method to get accuracy of model\n",
    "    #accuracy_score = rand_search.score(X_test, y_test) #for randomised search\n",
    "    #accuracy_score = grid_search.score(X_test, y_test) #for grid search\n",
    "    accuracy_score = GBR.score(X_test, y_test)\n",
    "    print('Accuracy of Gradient Boosting Regression on test set: {:.2f}'.format(accuracy_score))\n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    #---------------------------------RMSE & R2------------------------------\n",
    "    evaluate(y_train, y_pred_train, 'Train')\n",
    "    evaluate(y_test, y_pred_test, 'Test')\n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "    #---------------------------------Best Param-----------------------------\n",
    "    #print(rand_search.best_params_) #best parameters of randomised search\n",
    "    #print(rand_search.best_score_) #best score of randomised search\n",
    "    \n",
    "    #print(grid_search.best_params_) #best parameters of randomised search\n",
    "    #print(grid_search.best_score_) #best score of grid search\n",
    "    #print('\\n')\n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    filename = '../Dashboard UI/Dashboard UI/model/GBR_model.sav'\n",
    "    pickle.dump(GBR, open(filename, 'wb')) #save GBR model\n",
    "    \n",
    "    #Colab use\n",
    "    #filename = 'GBR_model.sav'\n",
    "    #pickle.dump(GBR, open(filename, 'wb')) #save GBR model\n",
    "    #files.download('GBR_model.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e7207",
   "metadata": {},
   "source": [
    "# Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c76bc29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n",
      "Accuracy of Gradient Boosting Regression on test set: 0.83\n",
      "Train set RMSE:17.28698720544705, R2:0.827917948833811\n",
      "Test set RMSE:17.3438063091837, R2:0.8258075668883855\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = pca(train, test, train_y, test_y)\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "gradient_boosting_regressor(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f63bf0",
   "metadata": {},
   "source": [
    "{'learning_rate': 0.1, 'max_features': 'auto', 'n_estimators': 90, 'subsample': 1.0}\n",
    "{'max_depth': 5, 'min_samples_split': 1400}\n",
    "{'min_samples_leaf': 37}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76239eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
